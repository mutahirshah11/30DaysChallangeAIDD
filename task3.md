## ðŸŒŸ Part A â€” Research Questions

1. **What new improvements were introduced in Gemini 3.0?**  
Gemini 3.0 brings significant improvements, including advanced reasoning and long-horizon planning, which allow it to handle complex, multistep tasks more effectively. It can process very large contexts of up to one million tokens and offers enhanced understanding across images, documents, and spatial reasoning. Additionally, the "Deep Think" mode enables the model to provide more structured and thoughtful responses.

2. **How does Gemini 3.0 improve coding & automation workflows?**  
Gemini 3.0 makes coding and automation much easier through agentic coding, allowing it to plan, write, check, and execute code autonomously. It integrates seamlessly with the Google Antigravity IDE, letting agents interact directly with the editor, terminal, and browser. The model also supports structured outputs and multitool workflows, while the Gemini CLI can suggest shell commands to automate tasks, helping developers work more efficiently.

3. **How does Gemini 3.0 improve multimodal understanding?**  
The model demonstrates stronger reasoning across images, videos, and documents, and it can handle complex spatial reasoning, which is useful in robotics, XR, and UI design. Developers can also adjust visual fidelity depending on the task, making Gemini 3.0 highly versatile for multimodal applications.

4. **Name any two developer tools introduced with Gemini 3.0.**  
Two notable developer tools are the Google Antigravity IDE, which offers a smart, agentdriven development environment, and the Gemini CLI, a client side bash tool that helps automate workflows and system commands efficiently.



## ðŸŒŸ Part B â€” Practical Task

![alt text](<gemini 2.PNG>)


I am currently on the waitlist for Gemini 3.0, so I do not have access to update the model at this time. (can be seen in the screen shot)